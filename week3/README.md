# Assignment 2

## FOUNDATION EXERCISES
[Previous coursework from MSSE 642](https://github.com/b-a-merritt/msse642/tree/main/assignment2)

## ADVANCED EXERCISES

I doubt it is possible for a large language model (LLM) to understand it lacks sufficient knowledge to discuss topics. The article discusses Wittgenstein; borrowing from Wittgenstein, let's view what can be said from an LLM as a subset of what can be said altogether. Let's imagine that around that subset sits a fence beyond which the LLM could not communicate. Knowing where the fence is necessates where it is in relation to the unknown side. It is impossible for the LLM to know where the limit exists from inside its boundaries. Knowing where the boundary is would increase the size of the boundary beyond its current limits, thus implying a logical contradiction. 

Arguable, an LLM could infer (or be trained on) roughly where its boundaries are in the same way I can get the sense that organic chemistry would start getting difficult for me. That still not not negate an LLM's inability to know exactly where that fence would be and say, "Sorry, what you are asking about is too difficult for me."

An LLM would not be able to tell that it lacks sufficient capability (from training or otherwise) to discuss a topic. It will always bullshit because it cannot know when it does not know.

Humans are good at separating bullshit from reality for the same reason we bullshit in the first place. We bullshit because care about the impression we are making, regardless of the truth or falsity of what we say. We meta-think, so to speak. We separate bullshit in the same way: we think about thinking of statements (i.e., "This seems like bs to me" is a thought about the thoughts on a topic). 

AI cannot think about its "thoughts." It produces what is most statistically likely to be said to a prompt. 

Humans are the best guardrail against bullshit, despite so often being the biggest (for now) producers of it. 

In ChatGPT's own words:

>Beyond the guardrails around privacy and content policy, my “blind spots” tend to fall into a few broad categories:
>
>First, any domain that demands genuine, first‑hand sensory or embodied experience is inaccessible to me. I can describe what chocolate tastes like based on text I’ve seen, but I can’t actually taste it; I can talk about the color of the sunset, but I can’t see it. If you asked me to compare two paintings from my own viewing, I’d have no personal visual memory to draw on.
>
>Second, problems that require true causal intervention in the world—flipping a physical switch at exactly 3:03 PM Kyiv time, steering a robot arm around obstacles or playing the violin—are out of my reach. I can plan and strategize, but I can’t execute or adapt in real time.
> 
>Third, deeply introspective or “meta” questions about my own inner workings—why exactly I chose word A instead of word B, what hidden weight in my neural network tripped at token 47—are beyond my self‑knowledge. I can approximate an explanation in human‑readable terms, but I don’t have conscious access to my own parameters.
>
>Fourth, truly novel scientific or mathematical breakthroughs remain out of scope. I can walk through established algebra, calculus or proof techniques, but if you asked me to resolve an unsolved conjecture—say, the Riemann Hypothesis—my answer would be that it’s an open problem in mathematics, not that I can furnish a proof.
>
>Finally, I struggle when the reasoning chain exceeds my context window or token limits. Very long, multi‑step calculations—say, symbolically manipulating a 50‑step algebraic derivation without error—can overwhelm me. If algebra truly were beyond my capabilities, I’d respond something like:
>
>“I’m sorry, but solving that algebraic problem falls outside my current reasoning limits.”
>
>In short, tasks requiring lived experience, real‑world agency, self‑introspection at the network level, original breakthroughs or extremely long, error‑free derivations lie beyond what I can reliably handle.